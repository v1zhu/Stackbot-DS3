{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "import re\n",
    "import tqdm\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from scipy.sparse import save_npz, load_npz\n",
    "import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0          0\n",
       "Id                  0\n",
       "Tag                 0\n",
       "Score_question      0\n",
       "question          596\n",
       "Score_answer        0\n",
       "Body_answer         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Dataset/cleaned.csv')\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset/cleaned.csv')\n",
    "df = df.dropna() #drop nulls, seem to be caused from saving the csv, but it is only 600 rows out of 1 million\n",
    "unique_df = df[['question',\"Id\"]].drop_duplicates() #get all questions\n",
    "temp = unique_df['question'].str.lower().str.split() #split all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538903/538903 [00:07<00:00, 72589.27it/s]\n"
     ]
    }
   ],
   "source": [
    "counts = defaultdict(int)\n",
    "for doc in temp:\n",
    "    for word in set(doc):\n",
    "            counts[word] +=1\n",
    "idf = {word: np.log(len(temp)/counts[word]) for word in counts}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates idf dictonary. Casts each question to a set to avoid counting duplicates in a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538903/538903 [00:10<00:00, 51509.69it/s]\n"
     ]
    }
   ],
   "source": [
    "tf_idf_list = []\n",
    "for doc in temp:\n",
    "    tf = Counter(doc)\n",
    "    length = len(doc)\n",
    "    tf_idf_doc = {}\n",
    "    for word,count in tf.items():\n",
    "            tf_idf_doc[word] = (count/length) * idf[word]\n",
    "    tf_idf_list.append(tf_idf_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a list of dicts with one dict for every question. Each dict is calculated by first getting the counts for words in each sentences and computing the term frequency. Then we multiply by the idf for that word calculated previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = {word: i for i, word in enumerate(sorted(idf.keys()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols, data = [],[],[]\n",
    "for i, tf_idf_doc in enumerate(tf_idf_list):\n",
    "    for word,val in tf_idf_doc.items():\n",
    "        if word in idf.keys():\n",
    "            rows.append(i)  \n",
    "            cols.append(unique_words[word])\n",
    "            data.append(val)\n",
    "tf_idf_matrix = csr_matrix((data,(rows,cols)), shape = (len(tf_idf_list),len(unique_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a sparse matrix representation of the tf_idf. This is important because a full matrix or pandas df takes up way too much memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(unique_words.items()), columns=['word', 'index']).to_csv('Dataset/word_to_index.csv', index=False)\n",
    "pd.DataFrame(sorted(idf.items()), columns=['word', 'idf_score']).to_csv('Dataset/idf.csv', index=False) \n",
    "save_npz('Dataset/sparse_matrix.npz', tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves idf, tf_idf matrix, and unique words indices in the tf_idf matrix. Idf and tf_idf are needed to process user querys later. Unique words is important because dictonaries are not in the same order always after being saved. This lets us know which columns in the tf_idf_matrix correspond to which words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
